{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team\n",
    "- 10644931 Ansaldi Jacopo\n",
    "- XXXXXXXX Omar Emara\n",
    "\n",
    "## Preprocessing\n",
    "We use a ... and then ... and then ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "import numpy as np\n",
    "\n",
    "# import matplotlib and allow it to plot inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# seaborn can generate several warnings, we ignore them\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the datasets and inspect \n",
    "speed dataset is loaded by chunks because it is to big to be processed in one block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_exploration(dataset):\n",
    "    print(dataset.info())\n",
    "    print(dataset.isnull().sum())\n",
    "    print(dataset.head(100))\n",
    "\n",
    "SPEED_TRAIN_PATH = \"C:\\\\Users\\erica\\Desktop\\jacopo\\progetto dmtm\\speeds_train.csv.gz\"\n",
    "SENSORS_PATH = \"C:\\\\Users\\erica\\Desktop\\jacopo\\progetto dmtm\\sensors.csv.gz\"\n",
    "EVENT_TRAIN_PATH = \"C:\\\\Users\\erica\\Desktop\\jacopo\\progetto dmtm\\events_train.csv.gz\"\n",
    "\n",
    "speed_chunks = pd.read_csv(SPEED_TRAIN_PATH,\n",
    "                           chunksize=150000)  # Specifying chunk size for the speed dataset to be 150,000 rows\n",
    "sensors = pd.read_csv(SENSORS_PATH)\n",
    "events = pd.read_csv(EVENT_TRAIN_PATH)\n",
    "\n",
    "data_exploration(events)\n",
    "data_exploration(sensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "events preprocessing:\n",
    "    1-casting dates\n",
    "    2-downcasting some values in order to occupy less memory\n",
    "    3-check missing values and deal with them\n",
    "    4-rename some columns to avoid duplicates with other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(dt):\n",
    "    print(dt.isnull().sum())\n",
    "\n",
    "events['START_DATETIME_UTC'] = pd.to_datetime(events['START_DATETIME_UTC'])\n",
    "events['END_DATETIME_UTC'] = pd.to_datetime(events['END_DATETIME_UTC'])\n",
    "events['EVENT_DETAIL'] = events['EVENT_DETAIL'].astype(np.int32)\n",
    "events['KEY'] = events['KEY'].astype(np.int32)\n",
    "events['KM_END'] = events['KM_END'].astype(np.int32)\n",
    "events['KM_START'] = events['KM_START'].astype(np.int32)\n",
    "check_missing_values(events)\n",
    "#==> 24 missing values on event detail\n",
    "#since the percentage of missing values is really small(<0.1%) ==> delete all the rows with mv\n",
    "events = events.dropna()\n",
    "\n",
    "# Avoiding duplicate columns\n",
    "events.rename(columns={'KEY': 'KEY_EVENTS', 'KEY_2': 'KEY_2_EVENTS'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then for each speed dataset chunk we:\n",
    "    1)downcasted the values to occupy less memory\n",
    "    2)merge the chunk with the sensors dataset on the attributes \"key\" and \"km\" that identifies the sensor\n",
    "    3)merge the resulting dataframe with the events dataset basing on the keys, the km range and the time range scaled to each \n",
    "    each quarter hour like in the speed dataset. So we obtained a dataset in which for every quarter hour there are 4 columns\n",
    "    that specify if there is one or more events occuring (4 because is the max num of simultaneous events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_casting_data_types(speeds_chunk):\n",
    "    # Down-casting the data types of some columns to 32-bit in order to reduce memory usage\n",
    "    speeds_chunk['KM'] = speeds_chunk['KM'].astype(np.int32)\n",
    "    speeds_chunk['KEY'] = speeds_chunk['KEY'].astype(np.int32)\n",
    "    speeds_chunk['SPEED_AVG'] = speeds_chunk['SPEED_AVG'].astype(np.float32)\n",
    "    speeds_chunk['SPEED_SD'] = speeds_chunk['SPEED_SD'].astype(np.float32)\n",
    "    speeds_chunk['SPEED_MIN'] = speeds_chunk['SPEED_MIN'].astype(np.float32)\n",
    "    speeds_chunk['SPEED_MAX'] = speeds_chunk['SPEED_MAX'].astype(np.float32)\n",
    "    speeds_chunk['N_VEHICLES'] = speeds_chunk['N_VEHICLES'].astype(np.int32)\n",
    "    return speeds_chunk\n",
    "\n",
    "\n",
    "def merge_speed_and_sensors(speed_chunk, sensors):\n",
    "    # Merging speeds.train.csv.gz with sensors.csv.gz\n",
    "    speeds_sensors = pd.merge(speed_chunk, sensors, on=['KEY', 'KM']).drop_duplicates().reset_index(\n",
    "        drop=True)\n",
    "    return speeds_sensors\n",
    "\n",
    "def merge_SpeedSensors_and_events(speeds_chunk,events):\n",
    "    # Merging speeds_sensors with events_train.csv.gz\n",
    "\n",
    "    sensor_km = speeds_sensors.KM.values\n",
    "\n",
    "    event_km_start = events.KM_START.values\n",
    "    event_km_end = events.KM_END.values\n",
    "\n",
    "    speed_sensors_key = speeds_sensors.KEY.values\n",
    "    events_key = events.KEY_EVENTS.values\n",
    "\n",
    "    speeds_sensors_time = speeds_sensors.DATETIME_UTC.values\n",
    "    events_time_start = events.START_DATETIME_UTC.values\n",
    "    events_time_end = events.END_DATETIME_UTC.values\n",
    "\n",
    "    # The conditions for the merge\n",
    "    i, j = np.where(\n",
    "        (speed_sensors_key[:, None] == events_key) & (\n",
    "                ((sensor_km[:, None] >= event_km_start) & (sensor_km[:, None] <= event_km_end)) | (\n",
    "                (sensor_km[:, None] <= event_km_start) & (sensor_km[:, None] >= event_km_end))) & (\n",
    "                speeds_sensors_time[:, None] >= events_time_start) & (speeds_sensors_time[:, None] <= events_time_end))\n",
    "\n",
    "    # Performing the merge based on the above conditions\n",
    "    speeds_sensors_events = pd.DataFrame(\n",
    "        np.column_stack([speeds_sensors.values[i], events.values[j]]),\n",
    "        columns=speeds_sensors.columns.append(events.columns)\n",
    "    ).append(\n",
    "        speeds_sensors[~np.in1d(np.arange(len(speeds_sensors)), np.unique(i))],\n",
    "        ignore_index=True, sort=False\n",
    "    )\n",
    "    return speeds_sensors_events\n",
    "\n",
    "\n",
    "\n",
    "chunk_list = []  # append each chunk df here\n",
    "\n",
    "for chunk in speed_chunks:\n",
    "\n",
    "    chunk = down_casting_data_types(chunk)\n",
    "    chunk['DATETIME_UTC'] = pd.to_datetime(chunk['DATETIME_UTC'])\n",
    "\n",
    "    # perform merging\n",
    "    speeds_sensors = merge_speed_and_sensors(chunk,sensors)\n",
    "    # perform merging\n",
    "    chunk_processed = merge_SpeedSensors_and_events(speeds_sensors,events)\n",
    "\n",
    "    # Once the data processing is done, append the chunk to list\n",
    "    chunk_list.append(chunk_processed)\n",
    "\n",
    "# concat the list into dataframe\n",
    "dataset = pd.concat(chunk_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
