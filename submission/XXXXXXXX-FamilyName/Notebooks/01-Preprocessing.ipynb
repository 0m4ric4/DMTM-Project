{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team\n",
    "- 10644931 Ansaldi Jacopo\n",
    "- XXXXXXXX Omar Emara\n",
    "\n",
    "## Preprocessing\n",
    "We use a ... and then ... and then ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "import numpy as np\n",
    "\n",
    "# import matplotlib and allow it to plot inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# seaborn can generate several warnings, we ignore them\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the datasets and inspect \n",
    "speed dataset is loaded by chunks because it is to big to be processed in one block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_exploration(dataset):\n",
    "    print(dataset.info())\n",
    "    print(dataset.isnull().sum())\n",
    "    print(dataset.head(100))\n",
    "\n",
    "SPEED_TRAIN_PATH = \"C:\\\\Users\\erica\\Desktop\\jacopo\\progetto dmtm\\speeds_train.csv.gz\"\n",
    "SENSORS_PATH = \"C:\\\\Users\\erica\\Desktop\\jacopo\\progetto dmtm\\sensors.csv.gz\"\n",
    "EVENT_TRAIN_PATH = \"C:\\\\Users\\erica\\Desktop\\jacopo\\progetto dmtm\\events_train.csv.gz\"\n",
    "\n",
    "speed_chunks = pd.read_csv(SPEED_TRAIN_PATH,\n",
    "                           chunksize=150000)  # Specifying chunk size for the speed dataset to be 150,000 rows\n",
    "sensors = pd.read_csv(SENSORS_PATH)\n",
    "events = pd.read_csv(EVENT_TRAIN_PATH)\n",
    "\n",
    "data_exploration(events)\n",
    "data_exploration(sensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "events preprocessing:\n",
    "    1) casting dates\n",
    "    2) downcasting some values in order to occupy less memory\n",
    "    3) check missing values and deal with them\n",
    "    4) rename some columns to avoid duplicates with other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(dt):\n",
    "    print(dt.isnull().sum())\n",
    "\n",
    "events['START_DATETIME_UTC'] = pd.to_datetime(events['START_DATETIME_UTC'])\n",
    "events['END_DATETIME_UTC'] = pd.to_datetime(events['END_DATETIME_UTC'])\n",
    "events['EVENT_DETAIL'] = events['EVENT_DETAIL'].astype(np.int32)\n",
    "events['KEY'] = events['KEY'].astype(np.int32)\n",
    "events['KM_END'] = events['KM_END'].astype(np.int32)\n",
    "events['KM_START'] = events['KM_START'].astype(np.int32)\n",
    "check_missing_values(events)\n",
    "#==> 24 missing values on event detail\n",
    "#since the percentage of missing values is really small(<0.1%) ==> delete all the rows with mv\n",
    "events = events.dropna()\n",
    "\n",
    "# Avoiding duplicate columns\n",
    "events.rename(columns={'KEY': 'KEY_EVENTS', 'KEY_2': 'KEY_2_EVENTS'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then for each speed dataset chunk we:\n",
    "    1)downcasted the values to occupy less memory\n",
    "    2)merge the chunk with the sensors dataset on the attributes \"key\" and \"km\" that identifies the sensor\n",
    "    3)merge the resulting dataframe with the events dataset basing on the keys, the km range and the time range scaled to each \n",
    "    each quarter hour like in the speed dataset. So we obtained a dataset in which for every quarter hour there are 4 columns\n",
    "    that specify if there is one or more events occuring (4 because is the max num of simultaneous events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_casting_data_types(speeds_chunk):\n",
    "    # Down-casting the data types of some columns to 32-bit in order to reduce memory usage\n",
    "    speeds_chunk['KM'] = speeds_chunk['KM'].astype(np.int32)\n",
    "    speeds_chunk['KEY'] = speeds_chunk['KEY'].astype(np.int32)\n",
    "    speeds_chunk['SPEED_AVG'] = speeds_chunk['SPEED_AVG'].astype(np.float32)\n",
    "    speeds_chunk['SPEED_SD'] = speeds_chunk['SPEED_SD'].astype(np.float32)\n",
    "    speeds_chunk['SPEED_MIN'] = speeds_chunk['SPEED_MIN'].astype(np.float32)\n",
    "    speeds_chunk['SPEED_MAX'] = speeds_chunk['SPEED_MAX'].astype(np.float32)\n",
    "    speeds_chunk['N_VEHICLES'] = speeds_chunk['N_VEHICLES'].astype(np.int32)\n",
    "    return speeds_chunk\n",
    "\n",
    "\n",
    "def merge_speed_and_sensors(speed_chunk, sensors):\n",
    "    # Merging speeds.train.csv.gz with sensors.csv.gz\n",
    "    speeds_sensors = pd.merge(speed_chunk, sensors, on=['KEY', 'KM']).drop_duplicates().reset_index(\n",
    "        drop=True)\n",
    "    return speeds_sensors\n",
    "\n",
    "def merge_SpeedSensors_and_events(speeds_chunk,events):\n",
    "    # Merging speeds_sensors with events_train.csv.gz\n",
    "\n",
    "    sensor_km = speeds_sensors.KM.values\n",
    "\n",
    "    event_km_start = events.KM_START.values\n",
    "    event_km_end = events.KM_END.values\n",
    "\n",
    "    speed_sensors_key = speeds_sensors.KEY.values\n",
    "    events_key = events.KEY_EVENTS.values\n",
    "\n",
    "    speeds_sensors_time = speeds_sensors.DATETIME_UTC.values\n",
    "    events_time_start = events.START_DATETIME_UTC.values\n",
    "    events_time_end = events.END_DATETIME_UTC.values\n",
    "\n",
    "    # The conditions for the merge\n",
    "    i, j = np.where(\n",
    "        (speed_sensors_key[:, None] == events_key) & (\n",
    "                ((sensor_km[:, None] >= event_km_start) & (sensor_km[:, None] <= event_km_end)) | (\n",
    "                (sensor_km[:, None] <= event_km_start) & (sensor_km[:, None] >= event_km_end))) & (\n",
    "                speeds_sensors_time[:, None] >= events_time_start) & (speeds_sensors_time[:, None] <= events_time_end))\n",
    "\n",
    "    # Performing the merge based on the above conditions\n",
    "    speeds_sensors_events = pd.DataFrame(\n",
    "        np.column_stack([speeds_sensors.values[i], events.values[j]]),\n",
    "        columns=speeds_sensors.columns.append(events.columns)\n",
    "    ).append(\n",
    "        speeds_sensors[~np.in1d(np.arange(len(speeds_sensors)), np.unique(i))],\n",
    "        ignore_index=True, sort=False\n",
    "    )\n",
    "    return speeds_sensors_events\n",
    "\n",
    "\n",
    "\n",
    "chunk_list = []  # append each chunk df here\n",
    "\n",
    "for chunk in speed_chunks:\n",
    "\n",
    "    chunk = down_casting_data_types(chunk)\n",
    "    chunk['DATETIME_UTC'] = pd.to_datetime(chunk['DATETIME_UTC'])\n",
    "\n",
    "    # perform merging\n",
    "    speeds_sensors = merge_speed_and_sensors(chunk,sensors)\n",
    "    # perform merging\n",
    "    chunk_processed = merge_SpeedSensors_and_events(speeds_sensors,events)\n",
    "\n",
    "    # Once the data processing is done, append the chunk to list\n",
    "    chunk_list.append(chunk_processed)\n",
    "\n",
    "# concat the list into dataframe\n",
    "dataset = pd.concat(chunk_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we deleted the useless columns in term of predictions because their information are redundant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['DATETIME_UTC'] = pd.to_datetime(dataset['DATETIME_UTC'])\n",
    "# dataset['DATETIME_UTC'] = dataset['DATETIME_UTC'] + pd.to_timedelta(dataset.groupby('DATETIME_UTC').cumcount(), unit='ms')\n",
    "dataset.set_index('DATETIME_UTC')\n",
    "\n",
    "dataset.drop('START_DATETIME_UTC', axis=1, inplace=True)\n",
    "dataset.drop('END_DATETIME_UTC', axis=1, inplace=True)\n",
    "dataset.drop('EVENT_TYPE', axis=1, inplace=True)\n",
    "dataset.drop('KM_START', axis=1, inplace=True)\n",
    "dataset.drop('KM_END', axis=1, inplace=True)\n",
    "dataset.drop('KEY_EVENTS', axis=1, inplace=True)\n",
    "dataset.drop('KEY_2_EVENTS', axis=1, inplace=True)\n",
    "\n",
    "print(dataset.info())\n",
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dataset.groupby(\n",
    "    [\"KEY\", \"DATETIME_UTC\", \"KM\", \"SPEED_AVG\", \"SPEED_SD\", \"SPEED_MIN\", \"SPEED_MAX\", \"N_VEHICLES\", \"KEY_2\",\n",
    "     \"EMERGENCY_LANE\",\n",
    "     \"LANES\", \"ROAD_TYPE\"]).cumcount().add(1)\n",
    "dataset = dataset.set_index(\n",
    "    [\"KEY\", \"DATETIME_UTC\", \"KM\", \"SPEED_AVG\", \"SPEED_SD\", \"SPEED_MIN\", \"SPEED_MAX\", \"N_VEHICLES\", \"KEY_2\",\n",
    "     \"EMERGENCY_LANE\",\n",
    "     \"LANES\", \"ROAD_TYPE\", g]).unstack(fill_value='NO_EVENT').sort_index(axis=1, level=1)\n",
    "dataset.columns = [\"{}{}\".format(a, b) for a, b in dataset.columns]\n",
    "\n",
    "dataset['EVENT_TYPE1'].fillna('NO_EVENT', inplace=True)\n",
    "\n",
    "dataset = dataset.reset_index()\n",
    "\n",
    "dataset.sort_values(by=[\"KEY\", \"KM\", \"DATETIME_UTC\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we merged the dataset with the weather one\n",
    "to start we load the distances dataset and drop the na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pd.read_csv('distances.csv.gz', delimiter='|', names=['ColA', 'ColB'])\n",
    "distances.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before merging we need to know which station is the nearest to each sensor, so that we will use its data for weather conditions \n",
    "associated with that sensor.\n",
    "for doing this we computed a dataframe with pairs key_2 (identifier of the sensors) and id of the nearest station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_processed = pd.DataFrame(columns=['KEY_2', 'ID'])\n",
    "\n",
    "# Iterating over distances.cv.gz to find the nearest weather station (ID) to each sensor (KEY_2)\n",
    "\n",
    "for index, row in distances.iterrows():\n",
    "\n",
    "    # Extracting KEY_2\n",
    "    key = row[\"ColA\"].split(',')[0] + \"_\" + row[\"ColA\"].split(',')[1]\n",
    "\n",
    "    # Creating a tuple storing the weather station ID, distances\n",
    "    # Ex: (STATION_29,10,STATION_37,19,STATION_36,40,STATION_30,64,STATION_33,94)\n",
    "    dist_tuple = tuple(row[\"ColB\"].split(','))\n",
    "\n",
    "    # Setting the minimum distance to be that of first station in the tuple\n",
    "    min_index = 1\n",
    "    min = float(dist_tuple[min_index])\n",
    "    print(index)\n",
    "\n",
    "    # Iterating over the odd numbers in the tuple to find the nearest station starting index 3 (since index 1 is already the minimum till now)\n",
    "    for i in range(3, len(dist_tuple), 2):\n",
    "        if float(dist_tuple[i]) < min:\n",
    "            min = float(dist_tuple[i])\n",
    "            min_index = i\n",
    "        # print(i, dist_tuple[i])\n",
    "    distances_processed.loc[index] = [key] + [dist_tuple[min_index - 1]]\n",
    "\n",
    "print(distances_processed.info())\n",
    "# Sample of the output of distances_processed\n",
    "# KEY_2\tID\n",
    "# 278_662\tSTATION_29\n",
    "# 278_663\tSTATION_29\n",
    "# 278_664\tSTATION_29\n",
    "# 278_665\tSTATION_29\n",
    "# 278_666\tSTATION_29\n",
    "# 278_667\tSTATION_29\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can merge the computed distances_processed dataframe with the datasets with the data of speed,sensors and \n",
    "events so that we have the condition to merge it with the weather one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(dataset, distances_processed, on='KEY_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we loaded the weather dataset and merged it with the previous one on the id and key_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('weather_train.csv')\n",
    "\n",
    "# Converting DATETIME_UTC to datetime dtype\n",
    "dataset['DATETIME_UTC'] = pd.to_datetime(dataset['DATETIME_UTC'])\n",
    "weather['DATETIME_UTC'] = pd.to_datetime(weather['DATETIME_UTC'])\n",
    "\n",
    "# Merging on DATETIME_UTC requires both data-frames to be sorted on DATETIME_UTC\n",
    "df.sort_values(by='DATETIME_UTC', inplace=True)\n",
    "weather.sort_values(by='DATETIME_UTC', inplace=True)\n",
    "\n",
    "# Merging on the nearest time difference after combining by the weather station ID\n",
    "dataset_final = pd.merge_asof(df, weather, on='DATETIME_UTC', by='ID', direction='nearest')\n",
    "dataset_final.sort_values(by=[\"KEY\", \"KM\", \"DATETIME_UTC\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at the end we can save our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final.to_csv('train.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
