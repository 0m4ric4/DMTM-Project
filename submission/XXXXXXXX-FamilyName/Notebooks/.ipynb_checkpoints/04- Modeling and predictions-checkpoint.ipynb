{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and predictions\n",
    "\n",
    "in this notebook we show how we compute the models for predictions\n",
    "\n",
    "techinques: normalization, pca, lstm (with keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from collections import OrderedDict\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "dataframe_dict = pd.read_csv('dict_to_cast_events_when_loading.csv',delimiter = \";\")\n",
    "dictionary = {}\n",
    "for index, row in dataframe_dict.iterrows():\n",
    "    dictionary[row['EVENTO']] = row['TIPO']\n",
    "print(dictionary)\n",
    "\n",
    "train = pd.read_csv('train_final_after_weather_encoding.csv', dtype=dictionary)\n",
    "test = pd.read_csv('test_final_after_weather_encoding.csv', dtype=dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we decided to compute a model for every sensor, so first we make a list of all possible key_2 in the intersection of \n",
    "train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dfs = dict(tuple(train.groupby('KEY_2')))\n",
    "test_dfs = dict(tuple(test.groupby('KEY_2')))\n",
    "\n",
    "train_keys = sorted(train_dfs)\n",
    "test_keys = sorted(test_dfs)\n",
    "\n",
    "# We should only train keys that are present in both train and set sets\n",
    "to_be_trained_keys = list(set(train_keys) & set(test_keys))\n",
    "#sorts and removes duplicates\n",
    "to_be_trained_keys = list(OrderedDict.fromkeys(to_be_trained_keys))\n",
    "to_be_trained_keys = sorted(to_be_trained_keys)\n",
    "\n",
    "trained_keys = []\n",
    "\n",
    "print(to_be_trained_keys)\n",
    "print(\"Number of keys to be trained:\" + str(len(to_be_trained_keys)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we prepare a dataframe for saving the result in the correct format for the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(columns=['KEY', 'KM', 'DATETIME_UTC', 'PREDICTION_STEP', 'SPEED_AVG'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then for every key we normalize the data, apply pca and then compute a model and predict the results, saving them directly in a dataframe in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "def select_subdataframe(key,dataframe):\n",
    "    dataframe = dataframe[key]\n",
    "    dataframe.set_index('DATETIME_UTC', inplace=True)\n",
    "    dataframe.drop('KEY_2', axis=1, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "def normalization(dataframe):\n",
    "    values= dataframe.values\n",
    "    scaler = StandardScaler()\n",
    "    scaled_dataframe = scaler.fit_transform(values)\n",
    "    return scaled_dataframe\n",
    "\n",
    "def PCA(data, n_components):\n",
    "    pca = PCA(n_components)\n",
    "    scaled_data = pca.fit_transform(data)\n",
    "    explained_variance = pca.explained_variance_ratio\n",
    "    return scaled_data\n",
    "    \n",
    "def prepare_data_for_prediction(dataframe,timeSteps,features):\n",
    "    supervised.drop(supervised.columns[range(features + 1, (features*2) + 0)], axis=1, inplace=True)\n",
    "    supervised.drop(supervised.columns[range(features + 2, (features*2) + 1)], axis=1, inplace=True)\n",
    "    supervised.drop(supervised.columns[range(features + 3, (features*2) + 2)], axis=1, inplace=True)\n",
    "    supervised.drop(supervised.columns[range(features + 4, (features*2) + 3)], axis=1, inplace=True)\n",
    "    supervised= supervised.values\n",
    "    X = supervised[:, :features * timeSteps]\n",
    "    Y = supervised[:, features * timeSteps:]\n",
    "    X = X.reshape(X.shape[0], timeSteps, features)\n",
    "    return [X,Y]\n",
    "\n",
    "def print_results(y_pred,X_test,_n_pca_components):\n",
    "    y_true = []\n",
    "    y_predicted = []\n",
    "    for i in range(0, ahead):\n",
    "        y_pred_i = y_pred[:, i]\n",
    "        y_pred_i = y_pred_i.reshape(y_test.shape[0], 1)\n",
    "        inv_new = np.concatenate((y_test_i, X_test[:, -(n_pca_components-1):]), axis=1)\n",
    "        inv_new = scaler.inverse_transform(inv_new)\n",
    "        final_pred = inv_new[:, 0]\n",
    "        y_predicted.append(final_pred)\n",
    "        y_test_i = y_test[:, i]\n",
    "        y_test_i = y_test_i.reshape(len(y_test_i), 1)\n",
    "        inv_new = np.concatenate((y_test_i, X_test[:, -(n_pca_components-1):]), axis=1)\n",
    "        inv_new = scaler.inverse_transform(inv_new)\n",
    "        actual_pred = inv_new[:, 0]\n",
    "        y_true.append(actual_pred)\n",
    "        plt.plot(final_pred[:200], label=\"prediction\", c=\"b\")\n",
    "        plt.plot(actual_pred[:200], label=\"actual data\", c=\"r\")\n",
    "        plt.xlim(0, 100)\n",
    "        plt.ylim(0, 300)\n",
    "        plt.yticks([])\n",
    "        plt.xticks([])\n",
    "        plt.title(\"comparison between prediction and actual data\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        print(\"mean absolute error:\")\n",
    "        print(mean_absolute_error(final_pred, actual_pred))\n",
    "        print(\"mean squared error:\")\n",
    "        print(mean_squared_error(final_pred, actual_pred)))\n",
    "\n",
    "def compute_results_dataframe(results_df,test_df, ahead, key, y_predicted):\n",
    "    temp_df = pd.DataFrame(columns=['KEY', 'KM', 'DATETIME_UTC', 'PREDICTION_STEP', 'SPEED_AVG'])\n",
    "\n",
    "    i = 0\n",
    "    test_df_truncated = test_df.tail(-ahead)\n",
    "    for index, row in test_df_truncated.iterrows():\n",
    "        # index is datetime\n",
    "        k = test_df_truncated.index.get_loc(index)\n",
    "        for j in range(0, ahead):\n",
    "            print(key.split('_')[0], key.split('_')[1], index, str(j + 1), y_predicted[j][k])\n",
    "            temp_df.loc[i+j] = [key.split('_')[0]] + [key.split('_')[1]] + [str(index)] + [str(j+1)] + [str(y_predicted[j][k])]\n",
    "        i+=4\n",
    "    result_df = result_df.append(temp_df)\n",
    "    return results_df\n",
    "\n",
    "    \n",
    "for key in to_be_trained_keys:\n",
    "    #we compute the model and predictions only for sensors with more than 5 values, because\n",
    "    #making a 4 step ahead prediction with less than 4 data makes no sense \n",
    "    n_samples_train = len(train_dfs[key].index) \n",
    "    n_samples_test = len(test_dfs[key].index)    \n",
    "    \n",
    "    if (n_samples_train >= 5 and n_samples_test >= 5):\n",
    "        \n",
    "        #we select only the part of the dataframe corresponding to the actual key and we remove the key from\n",
    "        #the attributes\n",
    "        trained_keys.append(key)\n",
    "        train_df = select_subdataframe(key,train_dfs)\n",
    "        test_df = select_subdataframe(key,test_dfs)\n",
    "\n",
    "        #then we normalize the data \n",
    "        scaled_train = normalization(train_df)\n",
    "        scaled_test = normalization(test_df)\n",
    "        \n",
    "        #to work the num of components of pca need to be at least equal at the num of samples\n",
    "        n_components_for_pca = 20\n",
    "        if(n_samples_train) < 20 or (n_samples_test < 20) :\n",
    "            n_components_for_pca = min(n_samples_train, n_samples_test)\n",
    "          \n",
    "        scaled_train = pca(scaled_train, n_components_for_pca)\n",
    "        scaled_test = pca(scaled_test, n_components_for_pca)\n",
    "\n",
    "        \n",
    "        #then we select how many time steps ahead make the prediction and how many time steps consider to predict the model\n",
    "        timeSteps = 1\n",
    "        ahead = 4\n",
    "        \n",
    "        #then we convert the data into a supervised problem using a sliding windows approach\n",
    "        supervised_train = series_to_supervised(scaled_train, n_in=timeSteps, n_out=ahead)\n",
    "        supervised_test = series_to_supervised(scaled_test, n_in=timeSteps, n_out=ahead)\n",
    "\n",
    "\n",
    "        features_train = n_components_for_pca\n",
    "        features_test = n_components_for_pca\n",
    "\n",
    "      \n",
    "        training_data = prepare_data_for_prediction(supervise_train,timeSteps,features_train)\n",
    "        X_train = training_data[0]\n",
    "        y_train = training_data[1]\n",
    "        \n",
    "        test_data = prepare_data_for_prediction(supervise_test,timeSteps,features_test)\n",
    "        X_test = test_data[0]\n",
    "        y_test = test_data[1]\n",
    "\n",
    "        #then we build the model using keras\n",
    "        \n",
    "        NUM_NEURONS_FirstLayer = 80\n",
    "        NUM_NEURONS_SecondLayer = 50\n",
    "        EPOCHS = 30\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(NUM_NEURONS_FirstLayer, input_shape=(timeSteps, X_train.shape[2]), return_sequences=True))\n",
    "        model.add(LSTM(NUM_NEURONS_SecondLayer, input_shape=(NUM_NEURONS_FirstLayer, 1)))\n",
    "\n",
    "        model.add(Dense(ahead))\n",
    "        sgd = optimizers.SGD(lr=0.1, decay = 1e-6, momentum = 0.9, nesterov=True)\n",
    "        model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "\n",
    "        history = model.fit(X_train, y_train, epochs=EPOCHS, shuffle=True, batch_size=24,\n",
    "                            verbose=2)\n",
    "        model.save('model_' + str(key) + \".h5\")\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        X_test = X_test.reshape(X_test.shape[0], X_test.shape[2] * X_test.shape[1])\n",
    "\n",
    "        print_results(y_pred, X_test,n_components_for_pca)\n",
    "        \n",
    "        results_df = compute_results_dataframe(results_df,test_df, ahead, key, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at the end we save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('results.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
